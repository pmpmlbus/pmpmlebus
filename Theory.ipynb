{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bde76d85-eb2d-40b2-9239-8c4eaad1c649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import os\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import Sequential\n",
    "# from tensorflow.keras.layers import InputLayer, Conv2D, MaxPool2D, Dense, Flatten\n",
    "# import cv2\n",
    "# from sklearn.metrics import confusion_matrix, classification_report, f1_score, recall_score\n",
    "# import seaborn as sns\n",
    "# y_vals = {'Brad Pitt' : 0, 'Hugh Jackman' : 1, 'Johnny Depp' : 2, 'Leonardo DiCaprio' : 3, 'Robert Downey Jr' : 4, 'Tom Cruise' : 5, 'Tom Hanks' : 6, 'Will Smith' : 7}\n",
    "\n",
    "# # dir_to_find = 'Celebrity_Faces_Dataset'\n",
    "\n",
    "# # labels = os.listdir(f'./Face_recognition/{dir_to_find}/')\n",
    "# # print(labels)\n",
    "# # x = []\n",
    "# # y = []\n",
    "\n",
    "# # for i in labels:\n",
    "# #     print(i)\n",
    "# #     for j in os.listdir(f'./Face_recognition/{dir_to_find}/{i}'):\n",
    "# #         img = cv2.imread(f'./Face_recognition/{dir_to_find}/{i}/{j}', cv2.IMREAD_GRAYSCALE)\n",
    "# #         if img.shape[0] >= 300 and img.shape[1] >= 300:\n",
    "# #             img = cv2.resize(img, (200, 200), interpolation=cv2.INTER_LINEAR)\n",
    "# #             x.append(img)\n",
    "# #             y.append(y_vals[i])\n",
    "\n",
    "\n",
    "# # x = np.array(x)\n",
    "# # y = np.array(y)\n",
    "# def show_img(x, y):\n",
    "#     plt.gray()\n",
    "#     plt.title(str(y))\n",
    "#     plt.imshow(x)\n",
    "#  # np.save('./face_reg.npy' , x)\n",
    "# # np.save('./face_reg_labels.npy' , y)X_train = np.load('./face_reg.npy')\n",
    "# y_train = np.load('./face_reg_labels.npy')\n",
    "# X_train = np.load('./face_reg.npy')\n",
    "# dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(1000)\n",
    "# def splits(dataset, TRAIN_RATIO = 0.8, VAL_RATIO = 0.15, TEST_RATIO = 0.05):\n",
    "#     DATASET_SIZE = len(dataset)\n",
    "\n",
    "#     train_dataset = dataset.take(int(TRAIN_RATIO*DATASET_SIZE)).map(lambda x, y: \n",
    "#         (\n",
    "#             tf.reshape(x , (200 , 200 , 1)) \n",
    "#             , y\n",
    "#         )\n",
    "#     ).batch(32)\n",
    "\n",
    "#     val_test_dataset = dataset.skip(int(TRAIN_RATIO*DATASET_SIZE))\n",
    "\n",
    "#     val_dataset = val_test_dataset.take(int(VAL_RATIO*DATASET_SIZE)).map(lambda x, y: \n",
    "#         (\n",
    "#             tf.reshape(x , (200 , 200 , 1)) \n",
    "#             , y\n",
    "#         )\n",
    "#     ).batch(32)\n",
    "\n",
    "#     test_dataset = val_test_dataset.skip(int(VAL_RATIO*DATASET_SIZE)).map(lambda x, y: \n",
    "#         (\n",
    "#             tf.reshape(x , (200 , 200 , 1)) \n",
    "#             , y\n",
    "#         )\n",
    "#     )\n",
    "#     return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "\n",
    "# train_ds , val_ds , test_ds = splits(dataset)\n",
    "# model = Sequential([\n",
    "#     Conv2D(filters = 16, kernel_size = (3 , 3), strides = 1, padding = 'same', activation = 'relu', input_shape = (200, 200, 1)),\n",
    "#     MaxPool2D(pool_size = (2 , 2), strides = 2),\n",
    "    \n",
    "#     Conv2D(filters = 32, kernel_size = (3 , 3), strides = 1, padding = 'same', activation = 'relu'),\n",
    "#     MaxPool2D(pool_size = (2 , 2), strides = 2),\n",
    "\n",
    "#     Conv2D(filters = 64, kernel_size = (3 , 3), strides = 1, padding = 'same', activation = 'relu'),\n",
    "#     MaxPool2D(pool_size = (2 , 2), strides = 2),\n",
    "    \n",
    "#     Flatten(),\n",
    "#     Dense(8, activation = 'softmax')\n",
    "# ])\n",
    "\n",
    "# model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# history = model.fit(train_ds, validation_data = val_ds, epochs=20)\n",
    "# for x, y in test_ds.shuffle(1000).as_numpy_iterator():\n",
    "#     show_img(x , y)\n",
    "#     x = x.reshape(1, 200, 200, 1)\n",
    "#     predictions = model.predict(x, verbose=0)\n",
    "#     # Get predicted class and confidence\n",
    "#     predicted_class = np.argmax(predictions[0])\n",
    "#     confidence = predictions[0][predicted_class]\n",
    "#     print('class =', predicted_class, 'conf=' , confidence)\n",
    "#     break\n",
    "\n",
    "# model.save('face_reg.keras')\n",
    "\n",
    "# plt.figure(figsize=(16,10))\n",
    "# plt.subplot(2,1,1)\n",
    "# plt.plot(range(1, 20+1), history.history['accuracy'], label='Training Accuracy')\n",
    "# plt.plot(range(1, 20+1), history.history['val_accuracy'], label='Validation Accuracy')\n",
    "# plt.title('Training and Validation Accuracy')\n",
    "# plt.legend([\"Training Accuracy\",\"Validation Accuracy\"])\n",
    "# plt.subplot(2,1,2)\n",
    "# plt.plot(range(1, 20+1), history.history['loss'], label='Training Loss')\n",
    "# plt.plot(range(1, 20+1), history.history['val_loss'], label='Validation Loss')\n",
    "# plt.title('Training and Validation Loss')\n",
    "# plt.legend([\"Training Loss\",\"Validation Loss\"])\n",
    "# plt.show()\n",
    "\n",
    "# y_true = []\n",
    "# y_pred = []\n",
    "\n",
    "# for x, y in test_ds.as_numpy_iterator():  # Iterate through the test dataset\n",
    "#     y_true.append(y.item())   # Collect true labels\n",
    "#     x = x.reshape(1, 200, 200, 1)\n",
    "#     predictions = model.predict(x)  # Predict on the batch\n",
    "#     y_pred.append(np.argmax(predictions, axis=1)[0].item())\n",
    "\n",
    "# y_true = np.array(y_true)\n",
    "# y_pred = np.array(y_pred) \n",
    "\n",
    "# conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "# f1 = f1_score(y_true, y_pred, average='weighted')  # Weighted F1-score\n",
    "# recall = recall_score(y_true, y_pred, average='weighted')  # Weighted Recall\n",
    "\n",
    "# # Print metrics\n",
    "# print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "# print(f\"F1 Score: {f1}\")\n",
    "# print(f\"Recall: {recall}\")\n",
    "\n",
    "# # Detailed classification report\n",
    "# print(\"\\nClassification Report:\")\n",
    "# print(classification_report(y_true, y_pred))\n",
    "\n",
    "# # Step 3: Visualize confusion matrix\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_true), yticklabels=np.unique(y_true))\n",
    "# plt.xlabel('Predicted')\n",
    "# plt.ylabel('True')\n",
    "# plt.title('Confusion Matrix')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1e78aa-f000-4c1f-bfcb-69cd020829c6",
   "metadata": {},
   "source": [
    "Facial Recognition Theory\n",
    "\n",
    "I'll break down the theory and explain this code, which is a celebrity face recognition machine learning project using a Convolutional Neural Network (CNN).\n",
    "\n",
    "### Project Overview\n",
    "This code implements a machine learning model to recognize and classify images of eight different celebrities: Brad Pitt, Hugh Jackman, Johnny Depp, Leonardo DiCaprio, Robert Downey Jr., Tom Cruise, Tom Hanks, and Will Smith.\n",
    "\n",
    "### Key Components and Theory\n",
    "\n",
    "#### 1. Data Preprocessing\n",
    "- Uses OpenCV (cv2) to load and process grayscale images\n",
    "- Resizes images to a consistent 200x200 pixel resolution\n",
    "- Stores image data (X) and corresponding labels (y)\n",
    "- Labels are mapped to numeric values using a dictionary `y_vals`\n",
    "\n",
    "#### 2. Convolutional Neural Network (CNN) Architecture\n",
    "The model uses a sequential CNN with these layers:\n",
    "- First Convolutional Layer: \n",
    "  - 16 filters, 3x3 kernel\n",
    "  - ReLU activation\n",
    "  - Processes input images (200x200, single channel)\n",
    "- MaxPooling Layer: Reduces spatial dimensions\n",
    "- Second Convolutional Layer: 32 filters\n",
    "- Third Convolutional Layer: 64 filters\n",
    "- Flattening Layer: Converts 2D feature maps to 1D\n",
    "- Dense Output Layer: 8 neurons (one per celebrity), softmax activation for classification\n",
    "\n",
    "#### 3. Training Process\n",
    "- Uses Adam optimizer\n",
    "- Sparse categorical cross-entropy loss function\n",
    "- Splits data into training (80%), validation (15%), and test (5%) sets\n",
    "- Trains for 20 epochs\n",
    "- Monitors training and validation accuracy/loss\n",
    "\n",
    "#### 4. Model Evaluation\n",
    "Uses several metrics to assess performance:\n",
    "- Confusion Matrix: Shows prediction accuracy for each class\n",
    "- F1 Score: Harmonic mean of precision and recall\n",
    "- Recall Score: Proportion of actual positives correctly identified\n",
    "- Classification Report: Detailed performance metrics per class\n",
    "\n",
    "#### 5. Visualization\n",
    "Creates plots to show:\n",
    "- Training and validation accuracy over epochs\n",
    "- Training and validation loss over epochs\n",
    "- Confusion matrix heatmap\n",
    "\n",
    "### Key Machine Learning Concepts Demonstrated\n",
    "1. Convolutional Neural Networks\n",
    "2. Transfer Learning (feature extraction)\n",
    "3. Image classification\n",
    "4. Performance metrics\n",
    "5. Data splitting and cross-validation\n",
    "6. Model training and evaluation\n",
    "\n",
    "The code provides a comprehensive approach to building a celebrity face recognition system using deep learning techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d31295d-1805-4938-a3b8-5d862f98420f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import os\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import Sequential\n",
    "# from tensorflow.keras.layers import InputLayer, Conv2D, MaxPool2D, Dense, Flatten\n",
    "# import cv2\n",
    "# # y_vals = {'angry' : 0, 'fear' : 1, 'happy' : 2, 'surprise' : 3}\n",
    "# # dir_to_find = 'validation'\n",
    "\n",
    "# # labels = os.listdir(f'./gesture_2/images/{dir_to_find}/')\n",
    "# # x = []\n",
    "# # y = []\n",
    "\n",
    "# # for i in labels:\n",
    "# #     print(i)\n",
    "# #     for j in os.listdir(f'./gesture_2/images/{dir_to_find}/{i}'):\n",
    "# #         img = cv2.imread(f'./gesture_2/images/{dir_to_find}/{i}/{j}', cv2.IMREAD_GRAYSCALE)\n",
    "# #         x.append(img)\n",
    "# #         y.append(y_vals[i])\n",
    "\n",
    "\n",
    "# # x = np.array(x)\n",
    "# # y = np.array(y)\n",
    "# # np.save('./train_gesture.npy' , x)\n",
    "# # np.save('./train_gesture_labels.npy' , y)\n",
    "\n",
    "# # np.save('./validation_gesture.npy' , x)\n",
    "# # np.save('./validation_gesture_labels.npy' , y)\n",
    "# X_train = np.load('./train_gesture.npy')\n",
    "# y_train = np.load('./train_gesture_labels.npy')\n",
    "\n",
    "# val_x = np.load('./validation_gesture.npy')\n",
    "# val_y = np.load('./validation_gesture_labels.npy')\n",
    "# train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(1000)\n",
    "# val_ds = tf.data.Dataset.from_tensor_slices((val_x, val_y)).shuffle(1000)\n",
    "# def show_img(x, y):\n",
    "#     plt.gray()\n",
    "#     plt.title(str(y))\n",
    "#     plt.imshow(x)\n",
    "\n",
    "\n",
    "# def reshape_train(train_ds):\n",
    "#     return train_ds.map(lambda x, y: \n",
    "#         (\n",
    "#             tf.reshape(x , (48 , 48 , 1)) \n",
    "#             , y\n",
    "#         )\n",
    "#     ).batch(32)\n",
    "\n",
    "# def divide_into_train_and_val(val_ds_original, val_ratio = 0.8, test_ratio = 0.2):\n",
    "#     \"\"\"\n",
    "#         pass the train dataset we will use\n",
    "#         train as -> train and val\n",
    "#         train will be divided into\n",
    "#         80% train and 20% val\n",
    "#     \"\"\"\n",
    "#     DATASET_SIZE = len(val_ds_original)\n",
    "\n",
    "#     val_ds = val_ds_original.take(int(val_ratio * DATASET_SIZE)).map(lambda x, y: \n",
    "#         (\n",
    "#             tf.reshape(x , (48 , 48 , 1)) \n",
    "#             , y\n",
    "#         )\n",
    "#     ).batch(32)\n",
    "\n",
    "#     test_ds = val_ds_original.skip(int(val_ratio * DATASET_SIZE)).map(lambda x, y: \n",
    "#         (\n",
    "#             tf.reshape(x , (48 , 48 , 1)) \n",
    "#             , y\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "#     return val_ds , test_ds\n",
    "\n",
    "# val_ds , test_ds = divide_into_train_and_val(val_ds)\n",
    "# train_ds = reshape_train(train_ds)\n",
    "\n",
    "# model = Sequential([\n",
    "#     Conv2D(filters = 16, kernel_size = (3 , 3), strides = 1, padding = 'same', activation = 'relu', input_shape = (48, 48, 1)),\n",
    "#     MaxPool2D(pool_size = (2 , 2), strides = 2),\n",
    "    \n",
    "#     Conv2D(filters = 32, kernel_size = (3 , 3), strides = 1, padding = 'same', activation = 'relu'),\n",
    "#     MaxPool2D(pool_size = (2 , 2), strides = 2),\n",
    "\n",
    "#     Conv2D(filters = 64, kernel_size = (3 , 3), strides = 1, padding = 'same', activation = 'relu'),\n",
    "#     MaxPool2D(pool_size = (2 , 2), strides = 2),\n",
    "    \n",
    "#     Flatten(),\n",
    "#     Dense(4, activation = 'softmax')\n",
    "# ])\n",
    "\n",
    "# model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# model.fit(train_ds, validation_data = val_ds, epochs=20)\n",
    "\n",
    "# for x, y in test_ds.shuffle(1000).as_numpy_iterator():\n",
    "#     show_img(x , y)\n",
    "#     x = x.reshape(1, 48, 48, 1)\n",
    "#     predictions = model.predict(x, verbose=0)\n",
    "#     # Get predicted class and confidence\n",
    "#     predicted_class = np.argmax(predictions[0])\n",
    "#     confidence = predictions[0][predicted_class]\n",
    "#     print('class =', predicted_class, 'conf=' , confidence)\n",
    "#     break\n",
    "\n",
    "\n",
    "# # x = x.reshape(1, 28, 56, 1)\n",
    "# # predictions = model.predict(x, verbose=0)\n",
    "# # Get predicted class and confidence\n",
    "# # predicted_class = np.argmax(predictions[0])\n",
    "# # confidence = predictions[0][predicted_class]\n",
    "# # print('class =', predicted_class, 'conf=' , confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a126e8-cbac-48fc-8932-26b380824b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Project Overview\n",
    "Gesture_Detection\n",
    "This code implements an emotion recognition machine learning model using a Convolutional Neural Network (CNN) to classify images into four emotional states.\n",
    "\n",
    "### Key Components and Theory\n",
    "\n",
    "#### 1. Data Preparation\n",
    "- Loads pre-processed image data from NumPy files\n",
    "- Creates TensorFlow datasets\n",
    "- Shuffles data for randomization\n",
    "- Splits data into training, validation, and test sets\n",
    "\n",
    "#### 2. CNN Architecture\n",
    "Model Structure:\n",
    "- First Convolutional Layer:\n",
    "  - 16 filters\n",
    "  - 3x3 kernel\n",
    "  - ReLU activation\n",
    "  - Input shape: 48x48x1 (grayscale images)\n",
    "- First MaxPooling Layer: Reduces spatial dimensions\n",
    "- Second Convolutional Layer: 32 filters\n",
    "- Second MaxPooling Layer\n",
    "- Third Convolutional Layer: 64 filters\n",
    "- Third MaxPooling Layer\n",
    "- Flattening Layer: Converts 2D features to 1D\n",
    "- Dense Output Layer: 4 neurons (softmax activation)\n",
    "\n",
    "#### 3. Training Process\n",
    "- Optimizer: Adam\n",
    "- Loss Function: Sparse Categorical Cross-Entropy\n",
    "- Metrics: Accuracy\n",
    "- Training Duration: 20 epochs\n",
    "- Batch Size: 32\n",
    "\n",
    "#### 4. Prediction Workflow\n",
    "- Selects random test image\n",
    "- Displays image\n",
    "- Reshapes image for prediction\n",
    "- Generates prediction\n",
    "- Outputs predicted class and confidence score\n",
    "\n",
    "### Key Machine Learning Techniques\n",
    "1. Convolutional Neural Networks\n",
    "2. Image classification\n",
    "3. Transfer learning\n",
    "4. Data preprocessing\n",
    "5. Model training and evaluation\n",
    "\n",
    "The code provides a comprehensive approach to emotion recognition using deep learning techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b28227f-0eec-4f07-8f60-490fdb4906a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import Sequential\n",
    "# from tensorflow.keras.layers import InputLayer, Conv2D, MaxPool2D, Dense, Flatten, UpSampling2D\n",
    "\n",
    "# data = np.load('./Image_Classification/mnist_compressed.npz')\n",
    "\n",
    "# X_test, y_test, X_train, y_train =  data['test_images'], data['test_labels'], data['train_images'], data['train_labels']\n",
    "\n",
    "# X_train.shape\n",
    "\n",
    "# X_train = X_train.astype('float32') / 255\n",
    "# X_test = X_test.astype('float32') / 255\n",
    "\n",
    "# noise_factor = 0.6\n",
    "# x_train_noisy = X_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=X_train.shape)\n",
    "# x_test_noisy = X_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=X_test.shape)\n",
    "\n",
    "# x_train_noisy = np.reshape(x_train_noisy, (len(x_train_noisy), 28, 56, 1))\n",
    "# x_test_noisy = np.reshape(x_test_noisy, (len(x_test_noisy), 28, 56, 1))\n",
    "# X_train = np.reshape(X_train, (len(X_train), 28, 56, 1))\n",
    "# X_test = np.reshape(X_test, (len(X_test), 28, 56, 1))\n",
    "\n",
    "# x_train_noisy = np.clip(x_train_noisy, 0., 1.)\n",
    "# x_test_noisy = np.clip(x_test_noisy, 0., 1.)\n",
    "\n",
    "# fig, axes = plt.subplots(1, 2, figsize=(20, 5))\n",
    "\n",
    "# plt.gray()\n",
    "# axes[0].imshow(X_train[0, : ,: , :])\n",
    "# axes[1].imshow(x_train_noisy[0, :,: , :])\n",
    "\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# model = Sequential([\n",
    "#                     # encoder network\n",
    "#                     Conv2D(32, 3, activation='relu', padding='same', input_shape=(28, 56, 1)),\n",
    "#                     MaxPool2D(2, padding='same'),\n",
    "#                     Conv2D(16, 3, activation='relu', padding='same'),\n",
    "#                     MaxPool2D(2, padding='same'),\n",
    "#                     # decoder network\n",
    "#                     Conv2D(16, 3, activation='relu', padding='same'),\n",
    "#                     UpSampling2D(2),\n",
    "#                     Conv2D(32, 3, activation='relu', padding='same'),\n",
    "#                     UpSampling2D(2),\n",
    "#                     # output layer\n",
    "#                     Conv2D(1, 3, activation='sigmoid', padding='same')\n",
    "# ])\n",
    "\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# model.summary()\n",
    "\n",
    "# history = model.fit(x_train_noisy, X_train, epochs=20, batch_size=256, validation_data=(x_test_noisy, X_test))\n",
    "\n",
    "# model.save('./denoise_2.keras')\n",
    "\n",
    "# denoised_images = model.predict(x_test_noisy)\n",
    "\n",
    "# # Plot original, noisy, and denoised images\n",
    "# n = 5  # Number of images to display\n",
    "# plt.figure(figsize=(20, 6))\n",
    "# for i in range(n):\n",
    "#     # Original images\n",
    "#     ax = plt.subplot(3, n, i + 1)\n",
    "#     plt.imshow(X_test[i].reshape(28, 56), cmap='gray')\n",
    "#     plt.title(\"Original\")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "#     # Noisy images\n",
    "#     ax = plt.subplot(3, n, i + 1 + n)\n",
    "#     plt.imshow(x_test_noisy[i].reshape(28, 56), cmap='gray')\n",
    "#     plt.title(\"Noisy\")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "#     # Denoised images\n",
    "#     ax = plt.subplot(3, n, i + 1 + 2*n)\n",
    "#     plt.imshow(denoised_images[i].reshape(28, 56), cmap='gray')\n",
    "#     plt.title(\"Denoised\")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6bfc43-ddb8-4a83-866a-ef94dfffac66",
   "metadata": {},
   "outputs": [],
   "source": [
    "Denoising\n",
    "I'll provide a detailed explanation with definitions of key terms:\n",
    "\n",
    "### Autoencoder Definition\n",
    "An autoencoder is a neural network designed to learn efficient data representations (encoding) and then reconstruct the original input (decoding), typically used for dimensionality reduction or noise removal.\n",
    "\n",
    "### Detailed Explanation with Definitions\n",
    "\n",
    "#### 1. Data Preprocessing Techniques\n",
    "- **Normalization**: Scaling pixel values to a standard range (0-1)\n",
    "  - Converts raw pixel values to floating-point between 0 and 1\n",
    "- **Gaussian Noise**: Random variation added to image data\n",
    "  - Uses normal distribution to simulate image distortions\n",
    "  - Noise factor (0.6) determines intensity of added noise\n",
    "\n",
    "#### 2. Convolutional Neural Network (CNN) Architecture\n",
    "- **Convolution Layer**: Extracts features from input images\n",
    "  - Applies filters to detect patterns\n",
    "  - Reduces dimensionality and captures spatial hierarchies\n",
    "- **MaxPooling Layer**: Reduces spatial dimensions\n",
    "  - Selects maximum value in pooling window\n",
    "  - Helps prevent overfitting\n",
    "- **UpSampling Layer**: Increases spatial dimensions\n",
    "  - Reconstructs original image size after compression\n",
    "\n",
    "#### 3. Autoencoder Components\n",
    "- **Encoder**: Compresses input into compact representation\n",
    "  - Reduces image complexity\n",
    "  - Captures essential features\n",
    "- **Decoder**: Reconstructs original image from compressed representation\n",
    "  - Aims to minimize difference between original and reconstructed images\n",
    "\n",
    "#### 4. Training Process Definitions\n",
    "- **Adam Optimizer**: Adaptive learning rate optimization algorithm\n",
    "  - Combines advantages of AdaGrad and RMSProp\n",
    "- **Binary Cross-Entropy**: Loss function measuring difference between predicted and actual values\n",
    "- **Sigmoid Activation**: Converts outputs to probability-like values between 0 and 1\n",
    "\n",
    "### Key Machine Learning Concepts\n",
    "1. **Feature Extraction**: Identifying important patterns in data\n",
    "2. **Dimensionality Reduction**: Compressing data while preserving essential information\n",
    "3. **Image Restoration**: Removing noise and recovering original image details\n",
    "\n",
    "The code implements a sophisticated deep learning approach to image denoising, demonstrating advanced neural network techniques for image processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c180dae-4798-4a86-964d-a6791773635e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import Sequential\n",
    "# from tensorflow.keras.layers import InputLayer, Conv2D, MaxPool2D, Dense, Flatten\n",
    "# data = np.load('./Image_Classification/mnist_compressed.npz')\n",
    "\n",
    "# X_test, y_test, X_train, y_train =  data['test_images'], data['test_labels'], data['train_images'], data['train_labels']\n",
    "\n",
    "# X_train.shape\n",
    "\n",
    "# test_full_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "# train = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(10)\n",
    "\n",
    "# def show_img(x, y):\n",
    "#     plt.gray()\n",
    "#     plt.title(str(y))\n",
    "#     plt.imshow(x)\n",
    "\n",
    "\n",
    "# def divide_into_train_and_val(train_dataset_original, train_ratio = 0.8, val_ratio = 0.2):\n",
    "#     \"\"\"\n",
    "#         pass the train dataset we will use\n",
    "#         train as -> train and val\n",
    "#         train will be divided into\n",
    "#         80% train and 20% val\n",
    "#     \"\"\"\n",
    "#     DATASET_SIZE = len(train_dataset_original)\n",
    "\n",
    "#     train_dataset = train_dataset_original.take(int(train_ratio * DATASET_SIZE)).map(lambda x, y: \n",
    "#         (\n",
    "#             tf.reshape(x , (28 , 56 , 1)) \n",
    "#             , y\n",
    "#         )\n",
    "#     ).batch(32)\n",
    "\n",
    "#     val_dataset = train_dataset_original.skip(int(train_ratio * DATASET_SIZE)).map(lambda x, y: \n",
    "#         (\n",
    "#             tf.reshape(x , (28 , 56 , 1)) \n",
    "#             , y\n",
    "#         )\n",
    "#     ).batch(32)\n",
    "\n",
    "#     return train_dataset, val_dataset\n",
    "\n",
    "# def show_img(x, y):\n",
    "#     plt.gray()\n",
    "#     plt.title(str(y))\n",
    "#     plt.imshow(x)\n",
    "\n",
    "\n",
    "# def divide_into_train_and_val(train_dataset_original, train_ratio = 0.8, val_ratio = 0.2):\n",
    "#     \"\"\"\n",
    "#         pass the train dataset we will use\n",
    "#         train as -> train and val\n",
    "#         train will be divided into\n",
    "#         80% train and 20% val\n",
    "#     \"\"\"\n",
    "#     DATASET_SIZE = len(train_dataset_original)\n",
    "\n",
    "#     train_dataset = train_dataset_original.take(int(train_ratio * DATASET_SIZE)).map(lambda x, y: \n",
    "#         (\n",
    "#             tf.reshape(x , (28 , 56 , 1)) \n",
    "#             , y\n",
    "#         )\n",
    "#     ).batch(32)\n",
    "\n",
    "#     val_dataset = train_dataset_original.skip(int(train_ratio * DATASET_SIZE)).map(lambda x, y: \n",
    "#         (\n",
    "#             tf.reshape(x , (28 , 56 , 1)) \n",
    "#             , y\n",
    "#         )\n",
    "#     ).batch(32)\n",
    "\n",
    "#     return train_dataset, val_dataset\n",
    "\n",
    "# train_dataset , val_dataset = divide_into_train_and_val(train)\n",
    "\n",
    "# model = Sequential([\n",
    "#     Conv2D(filters = 8, kernel_size = (3 , 3), strides = 1, padding = 'same', activation = 'relu', input_shape = (28, 56, 1)),\n",
    "#     MaxPool2D(pool_size = (2 , 2), strides = 2),\n",
    "    \n",
    "#     Conv2D(filters = 16, kernel_size = (3 , 3), strides = 1, padding = 'same', activation = 'relu'),\n",
    "#     MaxPool2D(pool_size = (2 , 2), strides = 2),\n",
    "    \n",
    "#     Flatten(),\n",
    "#     Dense(100, activation = 'softmax')\n",
    "# ])\n",
    "\n",
    "# model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# model.fit(train_dataset, validation_data = val_dataset, epochs=20)\n",
    "\n",
    "# x = X_test[69]\n",
    "# y = y_test[69]\n",
    "# show_img(x , y)\n",
    "# x = x.reshape(1, 28, 56, 1)\n",
    "# predictions = model.predict(x, verbose=0)\n",
    "# # Get predicted class and confidence\n",
    "# predicted_class = np.argmax(predictions[0])\n",
    "# confidence = predictions[0][predicted_class]\n",
    "# print('class =', predicted_class, 'conf=' , confidence)\n",
    "\n",
    "# model.save('./mnist_trained_weights.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefbeef5-eb10-47dc-8c18-7860ca879709",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Comprehensive Explanation with Definitions\n",
    "Image classification\n",
    "#### 1. Data Preprocessing\n",
    "- **Dataset Loading**: \n",
    "  - Loads MNIST compressed dataset\n",
    "  - Extracts training and testing images/labels\n",
    "- **TensorFlow Dataset Creation**:\n",
    "  - Converts NumPy arrays to TensorFlow datasets\n",
    "  - Enables efficient data processing and model training\n",
    "\n",
    "#### 2. Data Splitting Function\n",
    "- **divide_into_train_and_val()**: \n",
    "  - Splits dataset into training and validation sets\n",
    "  - Default ratio: 80% training, 20% validation\n",
    "  - Reshapes images to 28x56x1 format\n",
    "  - Batches data for efficient processing\n",
    "\n",
    "#### 3. Convolutional Neural Network (CNN) Architecture\n",
    "- **Layer Composition**:\n",
    "  1. First Convolutional Layer:\n",
    "     - 8 filters\n",
    "     - 3x3 kernel size\n",
    "     - ReLU activation\n",
    "     - Preserves spatial dimensions\n",
    "  2. First MaxPooling Layer:\n",
    "     - Reduces spatial dimensions\n",
    "     - Helps extract key features\n",
    "  3. Second Convolutional Layer:\n",
    "     - 16 filters\n",
    "     - 3x3 kernel size\n",
    "     - ReLU activation\n",
    "  4. Second MaxPooling Layer\n",
    "  5. Flattening Layer:\n",
    "     - Converts 2D feature maps to 1D\n",
    "  6. Dense Output Layer:\n",
    "     - 100 neurons\n",
    "     - Softmax activation for multi-class classification\n",
    "\n",
    "#### 4. Training Process\n",
    "- **Optimizer**: Adam\n",
    "  - Adaptive learning rate optimization\n",
    "- **Loss Function**: Sparse Categorical Cross-Entropy\n",
    "  - Measures difference between predicted and actual labels\n",
    "- **Metrics**: Accuracy\n",
    "- **Training Duration**: 20 epochs\n",
    "- **Batch Size**: 32\n",
    "\n",
    "#### 5. Prediction Workflow\n",
    "- Selects specific test image\n",
    "- Visualizes original image\n",
    "- Generates prediction\n",
    "- Outputs:\n",
    "  1. Predicted class\n",
    "  2. Confidence score\n",
    "\n",
    "### Key Machine Learning Concepts\n",
    "1. Convolutional Neural Networks\n",
    "2. Image Classification\n",
    "3. Feature Extraction\n",
    "4. Transfer Learning\n",
    "5. Model Training and Evaluation\n",
    "\n",
    "### Definitions of Key Terms\n",
    "- **Convolution**: Mathematical operation extracting features from input\n",
    "- **MaxPooling**: Downsampling technique reducing spatial dimensions\n",
    "- **ReLU Activation**: Rectified Linear Unit, introduces non-linearity\n",
    "- **Softmax**: Converts raw scores to probability distribution\n",
    "- **Epoch**: Complete passage through entire training dataset\n",
    "- **Batch**: Subset of training data processed simultaneously\n",
    "\n",
    "The code demonstrates a sophisticated approach to handwritten digit recognition using deep learning techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ab6f5c-fdc9-4be7-bc1e-9cd94503f7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt \n",
    "\n",
    "# def scaling(image):\n",
    "#     tx= 1.5\n",
    "#     ty= 0.5\n",
    "#     height, width = image.shape[:2]\n",
    "#     scaled_height= int(height*ty)\n",
    "#     scaled_width= int(width*tx)\n",
    "#     scaled_image = cv2.resize(image,(scaled_width, scaled_height))\n",
    "#     return scaled_image\n",
    "\n",
    "# def translate(image):\n",
    "#     tx= 5\n",
    "#     ty= 10\n",
    "#     row, col = image.shape[:2]\n",
    "#     M = np.float32([[1,0,tx], [0,1,ty]])\n",
    "#     scaled_image = cv2.warpAffine(image, M, (col, row))\n",
    "#     return scaled_image\n",
    "    \n",
    "# def rotation(image):\n",
    "#     angle= 45\n",
    "#     row, col = image.shape[:2]\n",
    "#     M = cv2.getRotationMatrix2D((col/2, row/2), angle, 1)\n",
    "#     rotated_image = cv2.warpAffine(image, M, (col, row))\n",
    "#     return rotated_image\n",
    "\n",
    "# def shearing(image):\n",
    "#     tx= 0.5\n",
    "#     ty= 0.5\n",
    "#     row, col = image.shape[:2]\n",
    "#     M = np.float32([[1,tx,0], [ty,1,0]])\n",
    "#     sheared_image = cv2.warpAffine(image, M, (col, row))\n",
    "#     return sheared_image\n",
    "# def reflection(image):\n",
    "#     reflected_image = cv2.flip(image, 0)\n",
    "#     return reflected_image\n",
    "\n",
    "# def display(original, filtered):\n",
    "#     plt.figure(figsize=(10,5))\n",
    "\n",
    "#     plt.subplot(121)\n",
    "#     plt.imshow(original)\n",
    "#     plt.title(\"original\")\n",
    "\n",
    "#     plt.subplot(122)\n",
    "#     plt.imshow(filtered)\n",
    "#     plt.title(\"filtered\")\n",
    "\n",
    "#     plt.show()\n",
    "\n",
    "# def main():\n",
    "#     while True:\n",
    "#         img_path = input(\"enter the image path: \")\n",
    "#         image = cv2.imread(img_path)\n",
    "\n",
    "#         while True:\n",
    "#             print(\"1. scaling\")\n",
    "#             print(\"2. translate\")\n",
    "#             print(\"3. rotate\")\n",
    "#             print(\"4. shearing\")\n",
    "#             print(\"5. reflection\")\n",
    "#             print(\"6. load another image\")\n",
    "#             print(\"7. exit\")\n",
    "\n",
    "#             choice = input(\"enter your choice: \")\n",
    "#             if choice =='1':\n",
    "#                 filtered_image = scaling(image)\n",
    "#                 display(image, filtered_image)\n",
    "#             elif choice =='2':\n",
    "#                 filtered_image = translate(image)\n",
    "#                 display(image, filtered_image)\n",
    "#             elif choice =='3':\n",
    "#                 filtered_image = rotation(image)\n",
    "#                 display(image, filtered_image)\n",
    "#             elif choice =='4':\n",
    "#                 filtered_image = shearing(image)\n",
    "#                 display(image, filtered_image)\n",
    "#             elif choice =='5':\n",
    "#                 filtered_image = reflection(image)\n",
    "#                 display(image, filtered_image)\n",
    "#             elif choice =='6':\n",
    "#                 break\n",
    "#             elif choice =='7':\n",
    "#                 return \n",
    "#             else :\n",
    "#                 print(\"invalid choice\")\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ec6aa0-aa3e-4893-aa65-d3e10aaa456b",
   "metadata": {},
   "source": [
    "<!-- Image registration -->\n",
    "1. Geometric Transformations in Images\n",
    "Geometric transformations modify the position, orientation, or size of an image. They include scaling, translation, rotation, shearing, and reflection.\n",
    "\n",
    "2. Code Explanation\n",
    "Functions\n",
    "Each function implements a specific transformation on an image.\n",
    "\n",
    "scaling(image):\n",
    "\n",
    "Definition: Scaling resizes an image by changing its width and height by specified scaling factors \n",
    "𝑡\n",
    "𝑥\n",
    "t \n",
    "x\n",
    "​\n",
    "  (horizontal) and \n",
    "𝑡\n",
    "𝑦\n",
    "t \n",
    "y\n",
    "​\n",
    "  (vertical).\n",
    "Implementation:\n",
    "Calculates the new dimensions: scaled_width and scaled_height.\n",
    "Uses cv2.resize() to scale the image.\n",
    "Parameters:\n",
    "tx=1.5: Scales width by 1.5.\n",
    "ty=0.5: Scales height by 0.5.\n",
    "translate(image):\n",
    "\n",
    "Definition: Translation moves the image by shifting its pixels horizontally (\n",
    "𝑡\n",
    "𝑥\n",
    "t \n",
    "x\n",
    "​\n",
    " ) and vertically (\n",
    "𝑡\n",
    "𝑦\n",
    "t \n",
    "y\n",
    "​\n",
    " ).\n",
    "Implementation:\n",
    "Creates a translation matrix \n",
    "𝑀\n",
    "M using np.float32.\n",
    "Applies the transformation using cv2.warpAffine().\n",
    "rotation(image):\n",
    "\n",
    "Definition: Rotation rotates the image by a specified angle around a pivot point (typically the center of the image).\n",
    "Implementation:\n",
    "Computes the rotation matrix \n",
    "𝑀\n",
    "M using cv2.getRotationMatrix2D().\n",
    "Rotates the image with cv2.warpAffine().\n",
    "Parameter:\n",
    "angle=45°: Rotates the image 45 degrees.\n",
    "shearing(image):\n",
    "\n",
    "Definition: Shearing distorts the image along the x- or y-axis, introducing a \"slanting\" effect.\n",
    "Implementation:\n",
    "Creates a shearing matrix \n",
    "𝑀\n",
    "M with shear factors \n",
    "𝑡\n",
    "𝑥\n",
    "t \n",
    "x\n",
    "​\n",
    "  and \n",
    "𝑡\n",
    "𝑦\n",
    "t \n",
    "y\n",
    "​\n",
    " .\n",
    "Applies the transformation using cv2.warpAffine().\n",
    "reflection(image):\n",
    "\n",
    "Definition: Reflection flips the image along a specified axis (vertical or horizontal).\n",
    "Implementation:\n",
    "Uses cv2.flip(image, flipCode).\n",
    "flipCode=0: Flips the image vertically.\n",
    "display(original, filtered):\n",
    "\n",
    "Displays the original and transformed images side-by-side using Matplotlib.\n",
    "Parameters:\n",
    "original: The input image.\n",
    "filtered: The transformed image.\n",
    "Main Function\n",
    "The main function:\n",
    "\n",
    "Continuously asks the user for an image file path to load an image.\n",
    "Displays a menu of transformation options (scaling, translation, rotation, etc.).\n",
    "Applies the selected transformation and displays the results.\n",
    "Allows users to load a new image or exit the program.\n",
    "3. Important Concepts\n",
    "Image Representation:\n",
    "\n",
    "Images are stored as arrays of pixel values.\n",
    "Shape: height x width x channels.\n",
    "Transformation Matrices:\n",
    "\n",
    "Linear transformations like scaling, rotation, shearing, and translation are expressed as matrices.\n",
    "Example:\n",
    "Translation Matrix:\n",
    "𝑀\n",
    "=\n",
    "[\n",
    "1\n",
    "0\n",
    "𝑡\n",
    "𝑥\n",
    "0\n",
    "1\n",
    "𝑡\n",
    "𝑦\n",
    "]\n",
    "M=[ \n",
    "1\n",
    "0\n",
    "​\n",
    "  \n",
    "0\n",
    "1\n",
    "​\n",
    "  \n",
    "t \n",
    "x\n",
    "​\n",
    " \n",
    "t \n",
    "y\n",
    "​\n",
    " \n",
    "​\n",
    " ]\n",
    "Rotation Matrix:\n",
    "𝑀\n",
    "=\n",
    "[\n",
    "cos\n",
    "⁡\n",
    "(\n",
    "𝜃\n",
    ")\n",
    "−\n",
    "sin\n",
    "⁡\n",
    "(\n",
    "𝜃\n",
    ")\n",
    "𝑐\n",
    "𝑥\n",
    "sin\n",
    "⁡\n",
    "(\n",
    "𝜃\n",
    ")\n",
    "cos\n",
    "⁡\n",
    "(\n",
    "𝜃\n",
    ")\n",
    "𝑐\n",
    "𝑦\n",
    "]\n",
    "M=[ \n",
    "cos(θ)\n",
    "sin(θ)\n",
    "​\n",
    "  \n",
    "−sin(θ)\n",
    "cos(θ)\n",
    "​\n",
    "  \n",
    "c \n",
    "x\n",
    "​\n",
    " \n",
    "c \n",
    "y\n",
    "​\n",
    " \n",
    "​\n",
    " ]\n",
    "Affine Transformations:\n",
    "\n",
    "cv2.warpAffine() performs affine transformations, where straight lines remain straight and parallel lines stay parallel.\n",
    "4. Advantages of the Code\n",
    "Interactive: Users can choose transformations interactively.\n",
    "Dynamic: Works with different images and multiple transformations.\n",
    "Visualization: Clearly shows before-and-after images.\n",
    "5. Applications\n",
    "Augmenting datasets for machine learning.\n",
    "Image manipulation for artistic effects.\n",
    "Real-world applications like image registration and object detection preprocessing.\n",
    "This modular design makes the code adaptable for further transformations or automation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258bbd2e-7f10-40e9-b5ef-a49e5286a560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import layers, Model\n",
    "# from tensorflow.keras.optimizers import Adam,SGD\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Data Loading and Preprocessing\n",
    "# def load_and_preprocess_data(image_dir, mask_dir, img_size=(128, 128)):  # Reduced image size\n",
    "#     images = []\n",
    "#     masks = []\n",
    "#     filenames = []\n",
    "    \n",
    "#     for img_name in os.listdir(image_dir):\n",
    "#         img_path = os.path.join(image_dir, img_name)\n",
    "#         mask_path = os.path.join(mask_dir, f\"annotated_{img_name}\")\n",
    "        \n",
    "#         if os.path.exists(mask_path):\n",
    "#             # Read and resize image\n",
    "#             img = cv2.imread(img_path)\n",
    "#             img = cv2.resize(img, img_size)\n",
    "#             img = img / 255.0  # Normalize\n",
    "            \n",
    "#             # Read and resize mask\n",
    "#             mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "#             mask = cv2.resize(mask, img_size)\n",
    "#             mask = mask / 255.0  # Normalize\n",
    "#             mask = np.expand_dims(mask, axis=-1)\n",
    "            \n",
    "#             images.append(img)\n",
    "#             masks.append(mask)\n",
    "#             filenames.append(img_name)\n",
    "    \n",
    "#     return np.array(images), np.array(masks), filenames\n",
    "\n",
    "# # Minimal U-Net Model Definition\n",
    "# def build_minimal_unet(input_shape=(128, 128, 3)):  # Updated for smaller input size\n",
    "#     inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "#     # Contracting Path (Encoder)\n",
    "#     c1 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "#     p1 = layers.MaxPooling2D((2, 2))(c1)\n",
    "    \n",
    "#     c2 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(p1)\n",
    "#     p2 = layers.MaxPooling2D((2, 2))(c2)\n",
    "    \n",
    "#     # Bridge\n",
    "#     c3 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(p2)\n",
    "    \n",
    "#     # Expansive Path (Decoder)\n",
    "#     u4 = layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c3)\n",
    "#     u4 = layers.concatenate([u4, c2])\n",
    "#     c4 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(u4)\n",
    "    \n",
    "#     u5 = layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c4)\n",
    "#     u5 = layers.concatenate([u5, c1])\n",
    "#     c5 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(u5)\n",
    "    \n",
    "#     outputs = layers.Conv2D(1, (1, 1), activation='sigmoid')(c5)\n",
    "    \n",
    "#     model = Model(inputs=inputs, outputs=outputs)\n",
    "#     return model\n",
    "\n",
    "# # Training Function\n",
    "# def train_model(model, X_train, y_train, X_val, y_val, batch_size=8, epochs=1):\n",
    "#     model.compile(\n",
    "#         optimizer=Adam(learning_rate=0.005),  # Adam optimizer\n",
    "#         loss='binary_crossentropy',          # Binary crossentropy loss\n",
    "#         metrics=['accuracy']                 # Accuracy as a metric\n",
    "#     )\n",
    "    \n",
    "#     history = model.fit(\n",
    "#         X_train, y_train,\n",
    "#         batch_size=batch_size,\n",
    "#         epochs=epochs,\n",
    "#         validation_data=(X_val, y_val),\n",
    "#         callbacks=[\n",
    "#             tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True),\n",
    "#             tf.keras.callbacks.ReduceLROnPlateau(factor=0.2, patience=3)\n",
    "#         ]\n",
    "#     )\n",
    "#     return history\n",
    "    \n",
    "# # Main function\n",
    "# # Update main function to use the correct U-Net builder\n",
    "# def main():\n",
    "#     image_dir = r\"C:\\Users\\Manthan\\Desktop\\CV_DL_Practicals\\dog\\Dog Segmentation\\Images\"\n",
    "#     mask_dir = r\"C:\\Users\\Manthan\\Desktop\\CV_DL_Practicals\\dog\\Dog Segmentation\\Labels\"\n",
    "    \n",
    "#     # Load and preprocess data\n",
    "#     images, masks, filenames = load_and_preprocess_data(image_dir, mask_dir)\n",
    "    \n",
    "#     # Reserve 10 images for testing\n",
    "#     test_size = 10\n",
    "#     test_indices = np.random.choice(len(images), test_size, replace=False)\n",
    "    \n",
    "#     test_images = images[test_indices]\n",
    "#     test_masks = masks[test_indices]\n",
    "#     test_filenames = [filenames[i] for i in test_indices]\n",
    "   \n",
    "#     # Remaining for training\n",
    "#     train_indices = list(set(range(len(images))) - set(test_indices))\n",
    "#     train_images = images[train_indices]\n",
    "#     train_masks = masks[train_indices]\n",
    "    \n",
    "#     with tf.device('/CPU:0'):  # Or '/CPU:0' to force CPU\n",
    "#         model = build_minimal_unet(input_shape=(128, 128, 3))  # Correct input shape\n",
    "    \n",
    "#     # Train model\n",
    "#     history = train_model(model, train_images, train_masks, test_images, test_masks)\n",
    "    \n",
    "#     # Save the model\n",
    "#     # model.save('dog_segmentation_model.h5')\n",
    "#     predicted_masks = model.predict(test_images)\n",
    "#     predicted_masks = (predicted_masks > 0.5).astype(np.uint8)  # Threshold predictions\n",
    "    \n",
    "#     # Display 10 test images and their corresponding masks\n",
    "#     fig, axes = plt.subplots(test_size, 3, figsize=(15, 40))\n",
    "#     for i, (img, true_mask, pred_mask, fname) in enumerate(zip(test_images, test_masks, predicted_masks, test_filenames)):\n",
    "#         # Display test image\n",
    "#         axes[i, 0].imshow(img)\n",
    "#         axes[i, 0].set_title(f\"Test Image: {fname}\")\n",
    "#         axes[i, 0].axis('off')\n",
    "        \n",
    "#         # Display ground truth mask\n",
    "#         axes[i, 1].imshow(true_mask.squeeze(), cmap='gray')\n",
    "#         axes[i, 1].set_title(\"Ground Truth Mask\")\n",
    "#         axes[i, 1].axis('off')\n",
    "        \n",
    "#         # Display predicted mask\n",
    "#         axes[i, 2].imshow(pred_mask.squeeze(), cmap='gray')\n",
    "#         axes[i, 2].set_title(\"Predicted Mask\")\n",
    "#         axes[i, 2].axis('off')\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebca507b-7cbc-4913-a4ce-5c576107b19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Semantic Segmentation\n",
    "I'll break down the theory and definition of this code, which is a Python script for performing image segmentation using a U-Net model, specifically for dog image segmentation.\n",
    "\n",
    "## Key Theoretical Concepts\n",
    "\n",
    "### 1. Image Segmentation\n",
    "- Process of dividing an image into multiple segments or objects\n",
    "- Goal: Identify and separate different regions within an image\n",
    "- In this code, the goal is to segment dogs from their background\n",
    "\n",
    "### 2. U-Net Architecture\n",
    "- Convolutional Neural Network (CNN) designed for image segmentation\n",
    "- Unique \"U-shaped\" architecture with:\n",
    "  - Contracting path (encoder): Captures context\n",
    "  - Expansive path (decoder): Enables precise localization\n",
    "  - Skip connections between encoder and decoder layers\n",
    "- Excellent for semantic segmentation tasks with limited data\n",
    "\n",
    "### 3. Model Components\n",
    "\n",
    "#### Data Preprocessing\n",
    "- Loads images and corresponding masks\n",
    "- Resizes images to fixed dimensions (128x128)\n",
    "- Normalizes pixel values to [0, 1] range\n",
    "- Prepares data for training\n",
    "\n",
    "#### Model Architecture\n",
    "- Input layer: 128x128x3 (RGB image)\n",
    "- Encoder stages:\n",
    "  - Convolutional layers extract features\n",
    "  - MaxPooling reduces spatial dimensions\n",
    "- Bridge layer: Connects encoder and decoder\n",
    "- Decoder stages:\n",
    "  - Transposed convolutions upsample features\n",
    "  - Skip connections preserve spatial information\n",
    "- Output layer: Sigmoid activation for binary segmentation\n",
    "\n",
    "#### Training Process\n",
    "- Uses binary cross-entropy loss\n",
    "- Adam optimizer\n",
    "- Early stopping and learning rate reduction\n",
    "- Metrics track model performance\n",
    "\n",
    "### 4. Key Techniques\n",
    "- Transfer learning approach\n",
    "- Data augmentation (implicit)\n",
    "- Binary segmentation mask generation\n",
    "- Visualization of results\n",
    "\n",
    "## Workflow\n",
    "1. Load and preprocess dog images\n",
    "2. Split data into training and testing sets\n",
    "3. Build U-Net model\n",
    "4. Train model\n",
    "5. Predict segmentation masks\n",
    "6. Visualize results\n",
    "\n",
    "The code demonstrates a practical implementation of deep learning for image segmentation using a compact U-Net architecture tailored for dog image analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235c4281-fcd5-4faa-a82d-11ff51be48fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import Sequential\n",
    "# from tensorflow.keras.layers import InputLayer, Conv2D, MaxPool2D, Dense, Flatten\n",
    "# from tensorflow.keras.models import load_model\n",
    "# from tensorflow.keras.models import Model\n",
    "# data = np.load('./Image_Classification/mnist_compressed.npz')\n",
    "\n",
    "# X_test, y_test, X_train, y_train =  data['test_images'], data['test_labels'], data['train_images'], data['train_labels']\n",
    "\n",
    "# def show_img(x, y):\n",
    "#     plt.gray()\n",
    "#     plt.title(str(y))\n",
    "#     plt.imshow(x)\n",
    "\n",
    "# model = load_model('./mnist_trained_weights.keras')\n",
    "\n",
    "# layer_names = [layer.name for layer in model.layers \n",
    "#                       if isinstance(layer, (tf.keras.layers.Conv2D, tf.keras.layers.MaxPooling2D))]\n",
    "\n",
    "# #we have 10k test so 0 - 9999\n",
    "# x = X_test[999]\n",
    "# y = y_test[999]\n",
    "# show_img(x , y)\n",
    "# x = x.reshape(1, 28, 56, 1)\n",
    "# predictions = model.predict(x, verbose=0)\n",
    "# # Get predicted class and confidence\n",
    "# predicted_class = np.argmax(predictions[0])\n",
    "# confidence = predictions[0][predicted_class]\n",
    "# print('class =', predicted_class, 'conf=' , confidence)\n",
    "\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "# desiredLayers = [0]\n",
    "# desiredOutputs = [model.layers[i].output for i in desiredLayers] \n",
    "\n",
    "# newModel = Model(model.inputs, desiredOutputs)\n",
    "\n",
    "# layer_0 = newModel.predict(x)\n",
    "\n",
    "# layer_0.shape\n",
    "\n",
    "# images = layer_0[0, :, :, :]\n",
    "\n",
    "# # Plotting the 8 grayscale images\n",
    "# fig, axes = plt.subplots(2, 4, figsize=(20, 5))\n",
    "\n",
    "# for i, ax in enumerate(axes.flat):\n",
    "#     ax.imshow(images[:, :, i], cmap='gray')\n",
    "#     ax.axis('off')\n",
    "#     ax.set_title(f\"Image {i+1}\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# desiredLayers = [1]\n",
    "# desiredOutputs = [model.layers[i].output for i in desiredLayers] \n",
    "\n",
    "# newModel = Model(model.inputs, desiredOutputs)\n",
    "\n",
    "# layer_1 = newModel.predict(x)\n",
    "\n",
    "# layer_1.shape\n",
    "\n",
    "# images = layer_1[0, :, :, :]\n",
    "\n",
    "# fig, axes = plt.subplots(2, 4, figsize=(20, 5))\n",
    "\n",
    "# for i, ax in enumerate(axes.flat):\n",
    "#     ax.imshow(images[:, :, i], cmap='gray')\n",
    "#     ax.axis('off')\n",
    "#     ax.set_title(f\"Image {i+1}\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# desiredLayers = [2]\n",
    "# desiredOutputs = [model.layers[i].output for i in desiredLayers] \n",
    "\n",
    "# newModel = Model(model.inputs, desiredOutputs)\n",
    "\n",
    "# layer_2 = newModel.predict(x)\n",
    "\n",
    "# layer_2.shape\n",
    "\n",
    "# images = layer_2[0, :, :, :]\n",
    "\n",
    "# fig, axes = plt.subplots(4, 4, figsize=(20, 5))\n",
    "\n",
    "# for i, ax in enumerate(axes.flat):\n",
    "#     ax.imshow(images[:, :, i], cmap='gray')\n",
    "#     ax.axis('off')\n",
    "#     ax.set_title(f\"Image {i+1}\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# desiredLayers = [3]\n",
    "# desiredOutputs = [model.layers[i].output for i in desiredLayers] \n",
    "\n",
    "# newModel = Model(model.inputs, desiredOutputs)\n",
    "\n",
    "# layer_3 = newModel.predict(x)\n",
    "\n",
    "# layer_3.shape\n",
    "\n",
    "# images = layer_2[0, :, :, :]\n",
    "\n",
    "# fig, axes = plt.subplots(4, 4, figsize=(20, 5))\n",
    "\n",
    "# for i, ax in enumerate(axes.flat):\n",
    "#     ax.imshow(images[:, :, i], cmap='gray')\n",
    "#     ax.axis('off')\n",
    "#     ax.set_title(f\"Image {i+1}\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# desiredLayers = [4]\n",
    "# desiredOutputs = [model.layers[i].output for i in desiredLayers] \n",
    "\n",
    "# newModel = Model(model.inputs, desiredOutputs)\n",
    "\n",
    "# layer_4 = newModel.predict(x)\n",
    "\n",
    "# layer_4.shape\n",
    "\n",
    "# flattented = layer_4.flatten()\n",
    "\n",
    "# plt.plot(flattented)\n",
    "\n",
    "# desiredLayers = [5]\n",
    "# desiredOutputs = [model.layers[i].output for i in desiredLayers] \n",
    "\n",
    "# newModel = Model(model.inputs, desiredOutputs)\n",
    "\n",
    "# layer_5 = newModel.predict(x)\n",
    "\n",
    "# layer_5.shape\n",
    "\n",
    "# flattented_output = layer_5.flatten()\n",
    "\n",
    "# flattented_output.argmax()\n",
    "\n",
    "# plt.plot(flattented_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046a1297-78a3-4741-b17d-129dc98aff4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Theoretical Explanation of the Code\n",
    "Visualise CNN\n",
    "### Purpose\n",
    "This code demonstrates deep learning model visualization and feature extraction for MNIST digit classification using TensorFlow and Keras.\n",
    "\n",
    "### Key Theoretical Concepts\n",
    "\n",
    "#### 1. Model Loading and Preprocessing\n",
    "- Loads pre-trained MNIST digit classification model\n",
    "- Loads compressed image dataset (MNIST)\n",
    "- Selects a single test image (index 999)\n",
    "\n",
    "#### 2. Visualization Techniques\n",
    "- Visualizes:\n",
    "  - Original input image\n",
    "  - Prediction results\n",
    "  - Intermediate layer outputs (feature maps)\n",
    "\n",
    "#### 3. Feature Extraction Process\n",
    "- Creates new models that extract outputs from specific layers\n",
    "- Techniques:\n",
    "  - Model.layers[] to access layer information\n",
    "  - Model() to create new models with specific input/output\n",
    "  - Allows inspection of neural network's internal representations\n",
    "\n",
    "#### 4. Layer Visualization Strategy\n",
    "- Iterates through different convolutional and pooling layers\n",
    "- Extracts feature maps for each layer\n",
    "- Plots feature maps as grayscale images\n",
    "\n",
    "### Detailed Layer Analysis\n",
    "\n",
    "#### Layer 0 (First Convolutional Layer)\n",
    "- Extracts basic, low-level features\n",
    "- 8 feature maps visualized\n",
    "- Captures simple edges, gradients, and primitive shapes\n",
    "\n",
    "#### Layer 1\n",
    "- More complex feature representations\n",
    "- Combines low-level features\n",
    "- Identifies more sophisticated patterns\n",
    "\n",
    "#### Layer 2\n",
    "- 16 feature maps\n",
    "- Higher-level abstract representations\n",
    "- Captures more complex digit characteristics\n",
    "\n",
    "#### Layer 3 and 4\n",
    "- Increasingly abstract feature representations\n",
    "- Closer to final classification decision\n",
    "\n",
    "### Technical Components\n",
    "- NumPy for array manipulation\n",
    "- Matplotlib for visualization\n",
    "- TensorFlow/Keras for deep learning model handling\n",
    "- Convolutional Neural Network (CNN) architecture\n",
    "\n",
    "### Key Functions\n",
    "- `show_img()`: Display image with label\n",
    "- `model.predict()`: Generate predictions\n",
    "- `Model()`: Create custom model for layer extraction\n",
    "- Plotting functions: Visualize feature maps\n",
    "\n",
    "### Workflow\n",
    "1. Load pre-trained model\n",
    "2. Select test image\n",
    "3. Generate prediction\n",
    "4. Extract layer outputs\n",
    "5. Visualize feature maps\n",
    "6. Analyze internal model representations\n",
    "\n",
    "The code provides insights into how Convolutional Neural Networks learn and represent visual information through progressive feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed7c17e-1f22-47c6-95f9-cc40128179b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ultralytics import YOLO\n",
    "\n",
    "# import os\n",
    "# import torch\n",
    "# from ultralytics import YOLO\n",
    "\n",
    "# # Resolve OpenMP warning\n",
    "# os.environ['KMP_DUPLICATE_LIB_OK']='TRUE'\n",
    "\n",
    "# def train_yolo_model():\n",
    "#     # Detect and set device\n",
    "#     device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#     print(f\"Training on: {device}\")\n",
    "\n",
    "#     # Data path\n",
    "#     data_path = r\"C:\\Users\\ganes\\Downloads\\object detection-20241120T135806Z-001\\object detection\\Persian_Car_Plates_YOLOV8\\data.yaml\"\n",
    "    \n",
    "#     # Validate data path\n",
    "#     if not os.path.exists(data_path):\n",
    "#         raise FileNotFoundError(f\"Data YAML not found: {data_path}\")\n",
    "\n",
    "#     # Initialize and train model\n",
    "#     model = YOLO('yolov8n.pt')\n",
    "#     results = model.train(\n",
    "#         data=data_path,\n",
    "#         epochs=50,\n",
    "#         imgsz=640,\n",
    "#         batch=16,\n",
    "#         conf=0.5,\n",
    "#         device=device\n",
    "#     )\n",
    "\n",
    "#     return results\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     train_yolo_model()\n",
    "\n",
    "# 2.\n",
    "# import cv2\n",
    "# import math\n",
    "# from ultralytics import YOLO\n",
    "\n",
    "# # Image path\n",
    "# image_path = r\"C:\\Users\\ganes\\Downloads\\object detection-20241120T135806Z-001\\object detection\\Persian_Car_Plates_YOLOV8\\extra\\test.jpg\"\n",
    "\n",
    "# # Load model\n",
    "# model = YOLO(r\"runs/detect/train6/weights/best.pt\")\n",
    "# classnames = ['Plates']\n",
    "\n",
    "# # Read image\n",
    "# frame = cv2.imread(image_path)\n",
    "# frame = cv2.resize(frame, (1080,720))\n",
    "\n",
    "# # Detect objects\n",
    "# results = model(frame)\n",
    "\n",
    "# # Process detections\n",
    "# for info in results:\n",
    "#     parameters = info.boxes\n",
    "#     for box in parameters:\n",
    "#         x1, y1, x2, y2 = box.xyxy[0]\n",
    "#         x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "#         confidence = box.conf[0]\n",
    "#         class_detect = box.cls[0]\n",
    "#         class_detect = int(class_detect)\n",
    "#         class_detect = classnames[class_detect]\n",
    "#         conf = math.ceil(confidence * 100)\n",
    "        \n",
    "#         if conf > 50 and class_detect == 'Plates':\n",
    "#             cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "#             cv2.putText(frame, f'{class_detect} {conf}%', (x1, y1-10), \n",
    "#                         cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "# # Display image\n",
    "# cv2.imshow('Detected License Plates', frame)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9a044e-4a49-48e2-b565-0c903953e825",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Detailed Explanation of YOLO Object Detection Code\n",
    "YOLO \n",
    "### Code 1: YOLO Model Training\n",
    "\n",
    "#### Key Components\n",
    "- **Purpose**: Train a YOLO v8 model for Persian Car Plate Detection\n",
    "\n",
    "#### Key Functions and Techniques\n",
    "1. **Device Detection**\n",
    "   - Automatically selects GPU (CUDA) or CPU for training\n",
    "   - Optimizes training hardware resources\n",
    "\n",
    "2. **Model Initialization**\n",
    "   - Uses pre-trained YOLOv8 nano model (`yolov8n.pt`)\n",
    "   - Transfer learning approach\n",
    "\n",
    "3. **Training Configuration**\n",
    "   - Data source: `data.yaml` file\n",
    "   - Hyperparameters:\n",
    "     - Epochs: 50\n",
    "     - Image size: 640x640\n",
    "     - Batch size: 16\n",
    "     - Confidence threshold: 0.5\n",
    "\n",
    "### Code 2: Object Detection Inference\n",
    "\n",
    "#### Key Components\n",
    "- **Purpose**: Detect and visualize license plates in an image\n",
    "\n",
    "#### Detailed Workflow\n",
    "1. **Model Loading**\n",
    "   - Loads pre-trained weights from previous training\n",
    "   - Specifies class names ('Plates')\n",
    "\n",
    "2. **Image Processing**\n",
    "   - Reads image using OpenCV\n",
    "   - Resizes image to 1080x720\n",
    "\n",
    "3. **Detection Process**\n",
    "   - Runs YOLO inference on image\n",
    "   - Extracts bounding box coordinates\n",
    "   - Calculates detection confidence\n",
    "\n",
    "4. **Visualization Techniques**\n",
    "   - Draws green rectangles around detected plates\n",
    "   - Adds confidence percentage text\n",
    "   - Filters detections above 50% confidence\n",
    "\n",
    "#### Key Functions Used\n",
    "- `cv2.imread()`: Read image\n",
    "- `cv2.resize()`: Resize image\n",
    "- `cv2.rectangle()`: Draw bounding boxes\n",
    "- `cv2.putText()`: Add text annotations\n",
    "\n",
    "### Theoretical Concepts\n",
    "\n",
    "#### YOLO (You Only Look Once)\n",
    "- Real-time object detection algorithm\n",
    "- Divides image into grid cells\n",
    "- Predicts bounding boxes and class probabilities simultaneously\n",
    "\n",
    "#### Transfer Learning\n",
    "- Uses pre-trained model weights\n",
    "- Adapts to specific task (license plate detection)\n",
    "- Reduces training time and improves accuracy\n",
    "\n",
    "#### Confidence Thresholding\n",
    "- Filters low-confidence detections\n",
    "- Reduces false positives\n",
    "- Improves detection precision\n",
    "\n",
    "### Technical Highlights\n",
    "- Uses Ultralytics YOLO implementation\n",
    "- PyTorch backend\n",
    "- GPU acceleration support\n",
    "- OpenCV for image processing\n",
    "\n",
    "The code demonstrates a complete machine learning workflow: model training and inference for specialized object detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106946ac-bb0e-4641-8799-cd870458ce73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# import tensorflow as tf\n",
    "\n",
    "# class SentimentAnalyzer:\n",
    "#     def __init__(self, text_column='text', label_column='sentiment', max_length=100, max_words=10000):\n",
    "#         self.text_column = text_column\n",
    "#         self.label_column = label_column\n",
    "#         self.max_length = max_length\n",
    "#         self.max_words = max_words\n",
    "#         self.tokenizer = None\n",
    "#         self.label_encoder = None\n",
    "#         self.model = None\n",
    "        \n",
    "#     def prepare_data(self, df):\n",
    "#         # Clean data\n",
    "#         df = df.dropna(subset=[self.text_column, self.label_column])\n",
    "#         df[self.text_column] = df[self.text_column].astype(str)\n",
    "        \n",
    "#         # Encode labels\n",
    "#         self.label_encoder = LabelEncoder()\n",
    "#         labels = self.label_encoder.fit_transform(df[self.label_column])\n",
    "        \n",
    "#         # Tokenize texts\n",
    "#         self.tokenizer = Tokenizer(num_words=self.max_words)\n",
    "#         self.tokenizer.fit_on_texts(df[self.text_column])\n",
    "#         sequences = self.tokenizer.texts_to_sequences(df[self.text_column])\n",
    "#         padded_sequences = pad_sequences(sequences, maxlen=self.max_length, padding='post')\n",
    "        \n",
    "#         return train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
    "    \n",
    "#     def create_model(self):\n",
    "#         vocab_size = min(len(self.tokenizer.word_index) + 1, self.max_words)\n",
    "#         num_classes = len(self.label_encoder.classes_)\n",
    "        \n",
    "#         model = tf.keras.Sequential([\n",
    "#             tf.keras.layers.Embedding(vocab_size, 128, input_length=self.max_length),\n",
    "#             tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
    "#             tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "#             tf.keras.layers.Dense(64, activation='relu'),\n",
    "#             tf.keras.layers.Dropout(0.5),\n",
    "#             tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "#         ])\n",
    "        \n",
    "#         model.compile(\n",
    "#             optimizer='adam',\n",
    "#             loss='sparse_categorical_crossentropy',\n",
    "#             metrics=['accuracy']\n",
    "#         )\n",
    "#         return model\n",
    "    \n",
    "#     def train(self, df, epochs=20, batch_size=32):\n",
    "\n",
    "#         X_train, X_test, y_train, y_test = self.prepare_data(df)\n",
    "        \n",
    "#         # Create and train model\n",
    "#         self.model = self.create_model()\n",
    "        \n",
    "#         # Add early stopping\n",
    "#         early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "#             monitor='val_loss',\n",
    "#             patience=3,\n",
    "#             restore_best_weights=True\n",
    "#         )\n",
    "        \n",
    "#         history = self.model.fit(\n",
    "#             X_train, y_train,\n",
    "#             validation_data=(X_test, y_test),\n",
    "#             epochs=epochs,\n",
    "#             batch_size=batch_size,\n",
    "#             callbacks=[early_stopping]\n",
    "#         )\n",
    "#         return history\n",
    "    \n",
    "#     def predict(self, texts):\n",
    "#         if isinstance(texts, str):\n",
    "#             texts = [texts]\n",
    "            \n",
    "#         # Tokenize and pad\n",
    "#         sequences = self.tokenizer.texts_to_sequences(texts)\n",
    "#         padded_sequences = pad_sequences(sequences, maxlen=self.max_length, padding='post')\n",
    "        \n",
    "#         # Get predictions\n",
    "#         predictions = self.model.predict(padded_sequences)\n",
    "#         predicted_labels = self.label_encoder.inverse_transform(predictions.argmax(axis=1))\n",
    "        \n",
    "#         # Get confidence scores\n",
    "#         confidence_scores = predictions.max(axis=1)\n",
    "        \n",
    "#         return list(zip(predicted_labels, confidence_scores))\n",
    "\n",
    "\n",
    "\n",
    "# # lets g0!\n",
    "\n",
    "# df = pd.read_csv(r\"C:\\Users\\ganes\\Downloads\\Sentiment analysis_Social media post\\sentiment_analysis.csv\")\n",
    "# analyzer = SentimentAnalyzer(\n",
    "#     text_column='text',\n",
    "#     label_column='sentiment',\n",
    "#     max_length=1000,  \n",
    "#     max_words=10000  \n",
    "# )\n",
    "\n",
    "# # Train\n",
    "# history = analyzer.train(df, epochs=1)\n",
    "\n",
    "# # Make predictions\n",
    "# texts = [\n",
    "#     \"This game is amazing!\",\n",
    "#     \"The service was terrible\",\n",
    "#     \"It's okay, nothing special\"\n",
    "# ]\n",
    "# predictions = analyzer.predict(texts)\n",
    "# for text, (sentiment, confidence) in zip(texts, predictions):\n",
    "#     print(f\"Text: {text}\")\n",
    "#     print(f\"Sentiment: {sentiment} (Confidence: {confidence:.2f})\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacbdae3-3859-4072-aa1c-4d5659a412ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sentiment Analysis Code Detailed Explanation\n",
    "LSTM\n",
    "### Core Concept\n",
    "A machine learning system for automatically classifying sentiment in text using deep learning techniques.\n",
    "\n",
    "### Class: SentimentAnalyzer\n",
    "#### Purpose\n",
    "Comprehensive text sentiment classification with advanced preprocessing and model training\n",
    "\n",
    "### Key Components\n",
    "\n",
    "#### 1. Data Preprocessing Methods\n",
    "- **`prepare_data()`**\n",
    "  - Handles data cleaning\n",
    "  - Removes null values\n",
    "  - Converts text to sequences\n",
    "  - Splits data into training/testing sets\n",
    "  - Uses techniques:\n",
    "    - Label encoding\n",
    "    - Text tokenization\n",
    "    - Sequence padding\n",
    "\n",
    "#### 2. Deep Learning Model Architecture\n",
    "- **Embedding Layer**: 128-dimensional word embeddings\n",
    "- **Bidirectional LSTM Layers**: \n",
    "  - First layer: 64 units\n",
    "  - Second layer: 32 units\n",
    "- **Dense Layers**:\n",
    "  - Hidden layer: 64 neurons with ReLU activation\n",
    "  - Output layer: Softmax for multi-class classification\n",
    "- **Dropout**: 0.5 regularization\n",
    "\n",
    "#### 3. Training Process\n",
    "- **Optimizer**: Adam\n",
    "- **Loss Function**: Sparse Categorical Crossentropy\n",
    "- **Early Stopping**: Prevents overfitting\n",
    "- **Validation Split**: 20% of data\n",
    "\n",
    "### Technical Techniques\n",
    "- Transfer learning via word embeddings\n",
    "- Sequence modeling with LSTM\n",
    "- Bidirectional context understanding\n",
    "- Intelligent text preprocessing\n",
    "\n",
    "### Workflow\n",
    "1. Load sentiment dataset\n",
    "2. Initialize SentimentAnalyzer\n",
    "3. Train model\n",
    "4. Make predictions with confidence scores\n",
    "\n",
    "### Advanced Features\n",
    "- Dynamic vocabulary handling\n",
    "- Flexible text length management\n",
    "- Confidence score generation\n",
    "- Multi-class sentiment classification\n",
    "\n",
    "The code represents a sophisticated approach to understanding textual sentiment using deep learning neural networks.\n",
    "\n",
    "\n",
    "\n",
    "## LSTM (Long Short-Term Memory) Architecture Detailed Explanation\n",
    "\n",
    "### Core Architecture\n",
    "\n",
    "#### 1. Basic Structure\n",
    "- Specialized Recurrent Neural Network (RNN) architecture\n",
    "- Designed to solve long-term dependency problems\n",
    "\n",
    "### Key Components\n",
    "\n",
    "#### Cell State\n",
    "- Memory mechanism\n",
    "- Allows information to flow unchanged\n",
    "- Controls information preservation or removal\n",
    "\n",
    "#### Gates\n",
    "1. **Forget Gate**\n",
    "   - Decides what information to discard\n",
    "   - Sigmoid activation\n",
    "   - Outputs values between 0-1\n",
    "   - 0: completely forget\n",
    "   - 1: completely retain\n",
    "\n",
    "2. **Input Gate**\n",
    "   - Determines new information to store\n",
    "   - Two-part process:\n",
    "     - Sigmoid layer: What to update\n",
    "     - Tanh layer: Potential new information\n",
    "\n",
    "3. **Output Gate**\n",
    "   - Controls what parts of cell state to output\n",
    "   - Filters and transforms cell state\n",
    "\n",
    "### Mathematical Representation\n",
    "\n",
    "#### Core Equations\n",
    "- Forget Gate: f(t) = σ(Wf * [h(t-1), x(t)] + bf)\n",
    "- Input Gate: i(t) = σ(Wi * [h(t-1), x(t)] + bi)\n",
    "- Cell State Update: C(t) = tanh(Wc * [h(t-1), x(t)] + bc)\n",
    "- Output Gate: o(t) = σ(Wo * [h(t-1), x(t)] + bo)\n",
    "\n",
    "### Bidirectional LSTM Enhancements\n",
    "- Processes sequence in both forward and backward directions\n",
    "- Captures context from multiple perspectives\n",
    "- Improves understanding of complex sequences\n",
    "\n",
    "### Advantages\n",
    "- Handles long-term dependencies\n",
    "- Mitigates vanishing gradient problem\n",
    "- Flexible information storage and retrieval\n",
    "- Effective for sequential data processing\n",
    "\n",
    "### Applications\n",
    "- Natural Language Processing\n",
    "- Time Series Prediction\n",
    "- Speech Recognition\n",
    "- Sentiment Analysis\n",
    "- Machine Translation\n",
    "\n",
    "### Performance Characteristics\n",
    "- Computationally intensive\n",
    "- Requires significant training data\n",
    "- Excellent for context-dependent tasks\n",
    "\n",
    "The LSTM architecture provides a sophisticated mechanism for understanding and processing sequential information with remarkable efficiency and depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed935821-818d-438c-b301-136c983d757a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ##### import cv2\n",
    "# # import numpy as np \n",
    "# # import matplotlib.pyplot as plt \n",
    "\n",
    "# # def sobel_filter(image):\n",
    "# #     #convert image to gray scale \n",
    "# #     gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "# #     sobelX = cv2.Sobel(gray,cv2.CV_64F,1, 0, ksize=3)\n",
    "# #     sobelY = cv2.Sobel(gray,cv2.CV_64F, 0, 1, ksize=3)\n",
    "# #     magnitude= np.sqrt(sobelX**2 + sobelY**2)\n",
    "# #     #mormalize it \n",
    "# #     magnitude= np.uint8(magnitude*255/np.max(magnitude))\n",
    "# #     return magnitude\n",
    "# # def gaussian(image, ksize=5):\n",
    "# #     return cv2.GaussianBlur(image, (ksize, ksize), 0)\n",
    "# # def median(image, ksize=5):\n",
    "# #     return cv2.medianBlur(image, ksize)\n",
    "\n",
    "# # def display(original, filtered):\n",
    "# #     plt.figure(figsize=(10, 5))\n",
    "\n",
    "# #     plt.subplot(121)\n",
    "# #     plt.imshow(original)\n",
    "# #     plt.title(\"original image\")\n",
    "\n",
    "# #     plt.subplot(122)\n",
    "# #     plt.imshow(filtered)\n",
    "# #     plt.title(\"filtered image\")\n",
    "\n",
    "# #     plt.show()\n",
    "\n",
    "# # def main():\n",
    "# #     while True:\n",
    "        \n",
    "# #         img_path= input(\"enter image path: \")\n",
    "# #         image = cv2.imread(img_path)\n",
    "# #         while True:\n",
    "# #             print(\"1. sobel filter\")\n",
    "# #             print(\"2. gaussian filter\")\n",
    "# #             print(\"3. median filter\")\n",
    "# #             print(\"4. load another image\")\n",
    "# #             print(\"5. exit \")\n",
    "\n",
    "# #             choice= input(\"enter your choice: \")\n",
    "# #             if choice== '1':\n",
    "# #                 filtered = sobel_filter(image)\n",
    "# #                 display(image, filtered)\n",
    "# #             elif choice == '2':\n",
    "# #                 filtered = gaussian(image)\n",
    "# #                 display(image, filtered)\n",
    "# #             elif choice == '3':\n",
    "# #                 filtered = median(image)\n",
    "# #                 display(image, filtered)\n",
    "                 \n",
    "# #             elif choice == '4':\n",
    "# #                 break\n",
    "# #             elif choice == '5':\n",
    "# #                 return\n",
    "# #             else:\n",
    "# #                 print(\"Invalid choice !\")\n",
    "\n",
    "# # if __name__== \"__main__\":\n",
    "# #     main()\n",
    "\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# '''\n",
    "# TWO IMPLEMENTAIONS\n",
    "# 1. MANUAL IMPLEMENTATION\n",
    "# 2. LIBRARY IMPLEMENTATION \n",
    "# '''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Manual filter functions\n",
    "# def convolve2d(image, kernel):\n",
    "#     m, n = kernel.shape\n",
    "#     y, x = image.shape\n",
    "#     y = y - m + 1\n",
    "#     x = x - n + 1\n",
    "#     new_image = np.zeros((y, x))\n",
    "#     for i in range(y):\n",
    "#         for j in range(x):\n",
    "#             new_image[i][j] = np.sum(image[i:i+m, j:j+n] * kernel)\n",
    "#     return new_image\n",
    "\n",
    "# def to_grayscale(image):\n",
    "#     return cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# def sobel_filter(image):\n",
    "#     kernel_x = np.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]])\n",
    "#     kernel_y = np.array([[-1, -2, -1], [0, 0, 0], [1, 2, 1]])\n",
    "    \n",
    "#     grad_x = convolve2d(image, kernel_x)\n",
    "#     grad_y = convolve2d(image, kernel_y)\n",
    "    \n",
    "#     gradient_magnitude = np.sqrt(grad_x**2 + grad_y**2)\n",
    "#     gradient_magnitude = (gradient_magnitude / np.max(gradient_magnitude)) * 255\n",
    "    \n",
    "#     return gradient_magnitude.astype(np.uint8)\n",
    "\n",
    "# def median_filter(image, kernel_size=3):\n",
    "#     pad = kernel_size // 2\n",
    "#     padded_img = np.pad(image, ((pad, pad), (pad, pad)), mode='reflect')\n",
    "#     filtered_img = np.zeros_like(image)\n",
    "    \n",
    "#     for i in range(image.shape[0]):\n",
    "#         for j in range(image.shape[1]):\n",
    "#             window = padded_img[i:i+kernel_size, j:j+kernel_size]\n",
    "#             filtered_img[i, j] = np.median(window)\n",
    "    \n",
    "#     return filtered_img\n",
    "\n",
    "# def gaussian_kernel(size, sigma=1):\n",
    "#     kernel = np.fromfunction(\n",
    "#         lambda x, y: (1/(2*np.pi*sigma**2)) * np.exp(-((x-(size-1)/2)**2 + (y-(size-1)/2)**2)/(2*sigma**2)),\n",
    "#         (size, size)\n",
    "#     )\n",
    "#     return kernel / np.sum(kernel)\n",
    "\n",
    "# def gaussian_filter(image, kernel_size=5, sigma=1):\n",
    "#     kernel = gaussian_kernel(kernel_size, sigma)\n",
    "#     return convolve2d(image, kernel)\n",
    "\n",
    "# # Library-based filters\n",
    "# def apply_filters(image_path):\n",
    "#     img = cv2.imread(image_path)\n",
    "#     img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB for display\n",
    "\n",
    "#     # Convert to grayscale for manual processing\n",
    "#     gray_img = to_grayscale(img)\n",
    "\n",
    "#     # Apply manual Sobel filter\n",
    "#     sobel_img = sobel_filter(gray_img)\n",
    "\n",
    "#     # Apply manual Median filter\n",
    "#     median_img = median_filter(gray_img)\n",
    "\n",
    "#     # Apply library Gaussian filter (OpenCV)\n",
    "#     # Converting to grayscale for consistent Gaussian filtering (optional)\n",
    "#     gaussian_img = cv2.GaussianBlur(gray_img, (5, 5), 0)\n",
    "\n",
    "#     # Plot the results\n",
    "#     plt.figure(figsize=(12, 8))\n",
    "\n",
    "#     # Original Image\n",
    "#     plt.subplot(2, 2, 1)\n",
    "#     plt.title('Original Image')\n",
    "#     plt.imshow(img_rgb)\n",
    "#     plt.axis('off')\n",
    "\n",
    "#     # Sobel Filter (Edge Detection)\n",
    "#     plt.subplot(2, 2, 2)\n",
    "#     plt.title('Sobel Filter (Edge Detection)')\n",
    "#     plt.imshow(sobel_img, cmap='gray')\n",
    "#     plt.axis('off')\n",
    "\n",
    "#     # Median Filter (Noise Reduction)\n",
    "#     plt.subplot(2, 2, 3)\n",
    "#     plt.title('Median Filter (Noise Reduction)')\n",
    "#     plt.imshow(median_img, cmap='gray')\n",
    "#     plt.axis('off')\n",
    "\n",
    "#     # Gaussian Filter (Smoothing)\n",
    "#     plt.subplot(2, 2, 4)\n",
    "#     plt.title('Gaussian Filter (Smoothing)')\n",
    "#     plt.imshow(gaussian_img, cmap='gray')  # Ensure this is displayed in grayscale\n",
    "#     plt.axis('off')\n",
    "\n",
    "#     # Display all plots\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "# # Call the function\n",
    "# image_path = 'C:\\\\Users\\\\Manthan\\\\Desktop\\\\CV_DL_Practicals\\\\filtering_threshold_otsu_watershed_images_region_growing\\\\train_012.png'\n",
    "# apply_filters(image_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# LIBRARY CODE\n",
    "\n",
    "# def apply_filters(image_path):\n",
    "\n",
    "#     img = cv2.imread(image_path)\n",
    "#     img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "#     sobel_x = cv2.Sobel(img, cv2.CV_64F, 1, 0, ksize=3)\n",
    "#     sobel_y = cv2.Sobel(img, cv2.CV_64F, 0, 1, ksize=3)\n",
    "#     sobel_combined = np.sqrt(sobel_x**2 + sobel_y**2)\n",
    "    \n",
    "#     median_filtered = cv2.medianBlur(img, 5)\n",
    "    \n",
    "#     gaussian_filtered = cv2.GaussianBlur(img, (5,5), 0)\n",
    "    \n",
    "#     plt.figure(figsize=(12,8))\n",
    "    \n",
    "#     # Original Image\n",
    "#     plt.subplot(2,2,1)\n",
    "#     plt.title('Original Image')\n",
    "#     plt.imshow(img)\n",
    "#     plt.axis('off')\n",
    "    \n",
    "#     # Sobel Filter\n",
    "#     plt.subplot(2,2,2)\n",
    "#     plt.title('Sobel Filter (Edge Detection)')\n",
    "#     plt.imshow(sobel_combined, cmap='gray')\n",
    "#     plt.axis('off')\n",
    "    \n",
    "#     # Median Filter\n",
    "#     plt.subplot(2,2,3)\n",
    "#     plt.title('Median Filter (Noise Reduction)')\n",
    "#     plt.imshow(median_filtered)\n",
    "#     plt.axis('off')\n",
    "    \n",
    "#     # Gaussian Filter\n",
    "#     plt.subplot(2,2,4)\n",
    "#     plt.title('Gaussian Filter (Smoothing)')\n",
    "#     plt.imshow(gaussian_filtered)\n",
    "#     plt.axis('off')\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "# # Call the function \n",
    "# apply_filters(r'C:\\Users\\Manthan\\Desktop\\CV_DL_Practicals\\filtering_threshold_otsu_watershed_images_region_growing\\train_019.png')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4206470-2fa8-41a9-9df8-42572969156b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Image Filtering Code Detailed Explanation\n",
    "Assignment 1\n",
    "### Core Concepts\n",
    "Two implementations of image filtering techniques:\n",
    "1. Manual Implementation\n",
    "2. Library-based Implementation\n",
    "\n",
    "### Key Filtering Techniques\n",
    "\n",
    "#### 1. Sobel Filter (Edge Detection)\n",
    "- Detects image edges\n",
    "- Computes horizontal and vertical gradients\n",
    "- Highlights intensity changes\n",
    "\n",
    "##### Manual Implementation\n",
    "- Custom convolution with Sobel kernels\n",
    "- Gradient magnitude calculation\n",
    "- Normalizes output\n",
    "\n",
    "##### Library Implementation\n",
    "- Uses `cv2.Sobel()` function\n",
    "- Computes edge gradients\n",
    "- Combines X and Y gradients\n",
    "\n",
    "#### 2. Median Filter (Noise Reduction)\n",
    "- Removes salt-and-pepper noise\n",
    "- Replaces pixel with median of neighborhood\n",
    "\n",
    "##### Manual Implementation\n",
    "- Custom sliding window approach\n",
    "- Reflects image edges\n",
    "- Computes median for each pixel neighborhood\n",
    "\n",
    "##### Library Implementation\n",
    "- Uses `cv2.medianBlur()` function\n",
    "- Efficient noise removal\n",
    "\n",
    "#### 3. Gaussian Filter (Smoothing)\n",
    "- Reduces image noise\n",
    "- Applies Gaussian distribution kernel\n",
    "\n",
    "##### Manual Implementation\n",
    "- Generates custom Gaussian kernel\n",
    "- Applies convolution\n",
    "- Controls smoothing via kernel size and sigma\n",
    "\n",
    "##### Library Implementation\n",
    "- Uses `cv2.GaussianBlur()`\n",
    "- Quick and efficient smoothing\n",
    "\n",
    "### Advanced Techniques\n",
    "\n",
    "#### Convolution Function\n",
    "- Manual 2D convolution implementation\n",
    "- Handles kernel application\n",
    "- Supports various filter operations\n",
    "\n",
    "#### Image Preprocessing\n",
    "- Converts images to grayscale\n",
    "- Normalizes pixel intensities\n",
    "- Prepares images for filtering\n",
    "\n",
    "### Visualization\n",
    "- Matplotlib-based display\n",
    "- Shows original and filtered images\n",
    "- Supports multiple filter comparisons\n",
    "\n",
    "### Key Functions\n",
    "- `convolve2d()`: Manual convolution\n",
    "- `sobel_filter()`: Edge detection\n",
    "- `median_filter()`: Noise reduction\n",
    "- `gaussian_kernel()`: Custom kernel generation\n",
    "- `apply_filters()`: Comprehensive image processing\n",
    "\n",
    "### Workflow\n",
    "1. Load image\n",
    "2. Convert to appropriate color space\n",
    "3. Apply filters\n",
    "4. Visualize results\n",
    "\n",
    "The code demonstrates sophisticated image processing techniques with both manual and library-based implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c82ca17-e2ad-4801-ae38-01204b5262d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from scipy import ndimage\n",
    "\n",
    "# def display_results(images, titles):\n",
    "#     \"\"\"Display multiple images with their titles\"\"\"\n",
    "#     n = len(images)\n",
    "#     plt.figure(figsize=(15, 5))\n",
    "    \n",
    "#     for i in range(n):\n",
    "#         plt.subplot(1, n, i + 1)\n",
    "#         if len(images[i].shape) == 2:\n",
    "#             plt.imshow(images[i], cmap='gray')\n",
    "#         else:\n",
    "#             plt.imshow(cv2.cvtColor(images[i], cv2.COLOR_BGR2RGB))\n",
    "#         plt.title(titles[i])\n",
    "#         plt.axis('off')\n",
    "#     plt.show()\n",
    "\n",
    "# def threshold_segmentation(image):\n",
    "#     \"\"\"Apply Otsu's thresholding\"\"\"\n",
    "#     # Convert to grayscale if needed\n",
    "#     if len(image.shape) == 3:\n",
    "#         gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "#     else:\n",
    "#         gray = image\n",
    "        \n",
    "#     # Apply Otsu's thresholding\n",
    "#     _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "#     return thresh\n",
    "\n",
    "# def region_growing(image, seed_point=None):\n",
    "#     \"\"\"\n",
    "#     Region growing segmentation\n",
    "#     If seed_point is None, uses the center of the image as seed point\n",
    "#     \"\"\"\n",
    "#     if len(image.shape) == 3:\n",
    "#         gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "#     else:\n",
    "#         gray = image\n",
    "        \n",
    "#     if seed_point is None:\n",
    "#         seed_point = (gray.shape[0]//2, gray.shape[1]//2)\n",
    "    \n",
    "#     # Create a mask for the segmented region\n",
    "#     mask = np.zeros(gray.shape, dtype=np.uint8)\n",
    "    \n",
    "#     # Set seed point\n",
    "#     mask[seed_point[0], seed_point[1]] = 255\n",
    "    \n",
    "#     # Get seed point intensity\n",
    "#     seed_intensity = int(gray[seed_point[0], seed_point[1]])\n",
    "    \n",
    "#     # Threshold for intensity difference\n",
    "#     threshold = 30\n",
    "    \n",
    "#     # Region growing\n",
    "#     while True:\n",
    "#         mask_old = mask.copy()\n",
    "        \n",
    "#         # Dilate the current region\n",
    "#         kernel = np.ones((3,3), np.uint8)\n",
    "#         mask_dilated = cv2.dilate(mask, kernel, iterations=1)\n",
    "        \n",
    "#         # Get pixels that are in dilated mask but not in current mask\n",
    "#         new_pixels = cv2.subtract(mask_dilated, mask)\n",
    "        \n",
    "#         # Get intensities of these pixels in original image\n",
    "#         coords = np.where(new_pixels > 0)\n",
    "#         for x, y in zip(coords[0], coords[1]):\n",
    "#             if abs(int(gray[x,y]) - seed_intensity) < threshold:\n",
    "#                 mask[x,y] = 255\n",
    "        \n",
    "#         # If no new pixels were added, stop\n",
    "#         if np.array_equal(mask, mask_old):\n",
    "#             break\n",
    "    \n",
    "#     return mask\n",
    "\n",
    "# def watershed_segmentation(image):\n",
    "#     \"\"\"Apply watershed segmentation\"\"\"\n",
    "#     # Convert to grayscale if needed\n",
    "#     if len(image.shape) == 3:\n",
    "#         gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "#     else:\n",
    "#         gray = image\n",
    "    \n",
    "#     # Threshold\n",
    "#     _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "    \n",
    "#     # Noise removal\n",
    "#     kernel = np.ones((3,3), np.uint8)\n",
    "#     opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=2)\n",
    "    \n",
    "#     # Sure background area\n",
    "#     sure_bg = cv2.dilate(opening, kernel, iterations=3)\n",
    "    \n",
    "#     # Finding sure foreground area\n",
    "#     dist_transform = cv2.distanceTransform(opening, cv2.DIST_L2, 5)\n",
    "#     _, sure_fg = cv2.threshold(dist_transform, 0.7*dist_transform.max(), 255, 0)\n",
    "#     sure_fg = np.uint8(sure_fg)\n",
    "    \n",
    "#     # Finding unknown region\n",
    "#     unknown = cv2.subtract(sure_bg, sure_fg)\n",
    "    \n",
    "#     # Marker labelling\n",
    "#     _, markers = cv2.connectedComponents(sure_fg)\n",
    "#     markers = markers + 1\n",
    "#     markers[unknown == 255] = 0\n",
    "    \n",
    "#     # Apply watershed\n",
    "#     markers = cv2.watershed(cv2.cvtColor(gray, cv2.COLOR_GRAY2BGR), markers)\n",
    "    \n",
    "#     # Create the result image\n",
    "#     result = np.zeros_like(gray)\n",
    "#     result[markers == -1] = 255  # Boundaries\n",
    "    \n",
    "#     return result\n",
    "\n",
    "# def adaptive_thresholding(image):\n",
    "#     \"\"\"Apply adaptive thresholding\"\"\"\n",
    "#     if len(image.shape) == 3:\n",
    "#         gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "#     else:\n",
    "#         gray = image\n",
    "        \n",
    "#     return cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \n",
    "#                                 cv2.THRESH_BINARY, 11, 2)\n",
    "\n",
    "# def kmeans_segmentation(image, k=3):\n",
    "#     \"\"\"Apply K-means segmentation\"\"\"\n",
    "#     # Reshape image for k-means\n",
    "#     pixel_values = image.reshape((-1, 3))\n",
    "#     pixel_values = np.float32(pixel_values)\n",
    "    \n",
    "#     # Define criteria and apply kmeans\n",
    "#     criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.2)\n",
    "#     _, labels, centers = cv2.kmeans(pixel_values, k, None, criteria, 10, \n",
    "#                                   cv2.KMEANS_RANDOM_CENTERS)\n",
    "    \n",
    "#     # Convert back to uint8 and reshape\n",
    "#     centers = np.uint8(centers)\n",
    "#     segmented_image = centers[labels.flatten()]\n",
    "#     segmented_image = segmented_image.reshape(image.shape)\n",
    "    \n",
    "#     return segmented_image\n",
    "\n",
    "# def main():\n",
    "#     while True:\n",
    "#         # Get image path from user\n",
    "#         image_path = input(\"\\nEnter image path (or 'q' to quit): \")\n",
    "        \n",
    "#         if image_path.lower() == 'q':\n",
    "#             break\n",
    "            \n",
    "#         try:\n",
    "#             # Read image\n",
    "#             image = cv2.imread(image_path)\n",
    "#             if image is None:\n",
    "#                 raise ValueError(f\"Could not load image from {image_path}\")\n",
    "                \n",
    "#             while True:\n",
    "#                 print(\"\\nImage Segmentation Menu:\")\n",
    "#                 print(\"1. Basic Thresholding (Otsu's method)\")\n",
    "#                 print(\"2. Adaptive Thresholding\")\n",
    "#                 print(\"3. Region Growing\")\n",
    "#                 print(\"4. Watershed Segmentation\")\n",
    "#                 print(\"5. K-means Segmentation\")\n",
    "#                 print(\"6. Load Different Image\")\n",
    "#                 print(\"7. Exit\")\n",
    "                \n",
    "#                 choice = input(\"\\nEnter your choice (1-7): \")\n",
    "                \n",
    "#                 if choice == '1':\n",
    "#                     result = threshold_segmentation(image)\n",
    "#                     display_results([image, result], ['Original', 'Thresholded'])\n",
    "                    \n",
    "#                 elif choice == '2':\n",
    "#                     result = adaptive_thresholding(image)\n",
    "#                     display_results([image, result], ['Original', 'Adaptive Thresholded'])\n",
    "                    \n",
    "#                 elif choice == '3':\n",
    "#                     print(\"Click on the image to select seed point, or press any key to use center point\")\n",
    "#                     plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "#                     plt.axis('off')\n",
    "#                     seed_point = None\n",
    "#                     def onclick(event):\n",
    "#                         nonlocal seed_point\n",
    "#                         if event.xdata is not None and event.ydata is not None:\n",
    "#                             seed_point = (int(event.ydata), int(event.xdata))\n",
    "#                             plt.close()\n",
    "#                     fig = plt.gcf()\n",
    "#                     fig.canvas.mpl_connect('button_press_event', onclick)\n",
    "#                     plt.show()\n",
    "                    \n",
    "#                     result = region_growing(image, seed_point)\n",
    "#                     display_results([image, result], ['Original', 'Region Growing'])\n",
    "                    \n",
    "#                 elif choice == '4':\n",
    "#                     result = watershed_segmentation(image)\n",
    "#                     display_results([image, result], ['Original', 'Watershed'])\n",
    "                    \n",
    "#                 elif choice == '5':\n",
    "#                     k = int(input(\"Enter number of segments (2-10): \"))\n",
    "#                     k = max(2, min(10, k))  # Ensure k is between 2 and 10\n",
    "#                     result = kmeans_segmentation(image, k)\n",
    "#                     display_results([image, result], ['Original', f'K-means (k={k})'])\n",
    "                    \n",
    "#                 elif choice == '6':\n",
    "#                     break\n",
    "                    \n",
    "#                 elif choice == '7':\n",
    "#                     return\n",
    "                    \n",
    "#                 else:\n",
    "#                     print(\"Invalid choice! Please enter a number between 1 and 7.\")\n",
    "                    \n",
    "#         except Exception as e:\n",
    "#             print(f\"Error: {str(e)}\")\n",
    "#             print(\"Please try again with a valid image path.\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ddc624-ccdf-41dd-9ac2-bfc0694d77f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "I'll explain this Python script for image segmentation in detail:\n",
    "\n",
    "The script provides a comprehensive image segmentation tool with multiple algorithms:\n",
    "\n",
    "1. Overall Structure:\n",
    "- Uses OpenCV (cv2), NumPy, Matplotlib for image processing\n",
    "- Implements an interactive CLI-based image segmentation application\n",
    "- Offers multiple segmentation techniques\n",
    "\n",
    "2. Key Functions:\n",
    "\n",
    "a) `display_results(images, titles)`:\n",
    "- Displays multiple images side-by-side\n",
    "- Handles both grayscale and color images\n",
    "- Uses Matplotlib for visualization\n",
    "\n",
    "b) `threshold_segmentation(image)`:\n",
    "- Applies Otsu's thresholding method\n",
    "- Automatically determines optimal threshold value\n",
    "- Converts color images to grayscale if needed\n",
    "- Returns binary segmented image\n",
    "\n",
    "c) `region_growing(image, seed_point=None)`:\n",
    "- Advanced segmentation technique\n",
    "- Grows region from a seed point (center by default)\n",
    "- Uses intensity-based criteria to expand segmentation\n",
    "- Allows user to select custom seed point\n",
    "- Threshold controls region expansion\n",
    "\n",
    "d) `watershed_segmentation(image)`:\n",
    "- Complex segmentation algorithm\n",
    "- Uses distance transform and marker-based approach\n",
    "- Handles image boundaries and separates overlapping regions\n",
    "- Returns segmentation boundaries\n",
    "\n",
    "e) `adaptive_thresholding(image)`:\n",
    "- Uses Gaussian adaptive thresholding\n",
    "- Handles varying illumination across image\n",
    "- Calculates thresholds dynamically for different image regions\n",
    "\n",
    "f) `kmeans_segmentation(image, k=3)`:\n",
    "- Clustering-based segmentation\n",
    "- Divides image into K distinct segments\n",
    "- User can specify number of segments (2-10)\n",
    "- Uses k-means clustering algorithm\n",
    "\n",
    "3. Main Program Flow:\n",
    "- Interactive menu-driven interface\n",
    "- Allows repeated processing of images\n",
    "- Error handling for image loading\n",
    "- Provides multiple segmentation options\n",
    "- Supports loading different images\n",
    "- Allows exiting the program\n",
    "\n",
    "4. Visualization:\n",
    "- Uses Matplotlib for interactive image selection and display\n",
    "- Shows original and segmented images side-by-side\n",
    "- Supports different visualization methods for various segmentation techniques\n",
    "\n",
    "5. Key Libraries Used:\n",
    "- OpenCV (cv2): Image processing\n",
    "- NumPy: Numerical computations\n",
    "- Matplotlib: Visualization\n",
    "- SciPy: Scientific computing support\n",
    "\n",
    "The script provides a flexible, user-friendly tool for exploring different image segmentation techniques with interactive selection and visualization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
